{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4SuLyO1_saI",
        "outputId": "bf8fe872-40de-4e0f-ff30-fa1a3a39d3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1"
      ],
      "metadata": {
        "id": "Q_aji_FCPv62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "with open('Burbank.txt', 'r') as file: # reading the file\n",
        "    burbank_text = file.read()"
      ],
      "metadata": {
        "id": "_CZ1L43jLF73"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(burbank_text[:200]) # checking the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e6CWiHsLNfU",
        "outputId": "9236026c-fcf9-43d3-ec5f-7879306a2a7d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ï»¿Airport task force to consider possible actions to abate noise issues Members of the Southern San Fernando Valley Airplane Noise Task Force will have a little over a month to develop recommendations \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "blob = TextBlob(burbank_text) # creating a TextBlob object\n",
        "\n",
        "polarity = blob.sentiment.polarity # getting the polarity of the text\n",
        "\n",
        "print(polarity)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqRG_x2DLLgB",
        "outputId": "b42635b4-27eb-4850-9984-1d6d3a34e611"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.09869334480780263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The polarity is 0.09869334480780263 i.e. it is positive\n",
        "##hence, the probability of the sentiment going negative pretty low."
      ],
      "metadata": {
        "id": "AoYy3nMDN6OC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L3fPazpSLLcP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 2"
      ],
      "metadata": {
        "id": "rKHmkNGHPq9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subjectivity = blob.sentiment.subjectivity # getting the subjectivity score\n",
        "\n",
        "print(\"Polarity score:\", polarity)\n",
        "print(\"Subjectivity score:\", subjectivity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biWCoxBtAbvZ",
        "outputId": "9edcf0ac-af88-44c6-88a0-d5d928359432"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity score: 0.09869334480780263\n",
            "Subjectivity score: 0.3790877796901893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## With the polarity & subjectivity valuse being 0.09869334480780263 and 0.3790877796901893, we can say that the overall sentiment is positive and is more objective.\n"
      ],
      "metadata": {
        "id": "gAICLBTFPIGZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 3"
      ],
      "metadata": {
        "id": "Siy73Vwlk2LN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # i had to download these to avoid errors\n",
        "nltk.download('brown')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YNEdMUUSBJJ",
        "outputId": "6e916927-0182-4eb3-96c3-5685faaa8328"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PM_aJNoPjvu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import Word\n",
        "import string\n",
        "\n",
        "\n",
        "topics = [] # extracting the nouns/proper nouns from the text\n",
        "for wrd, tag in blob.tags: # using .tags functionality from textblob to get nouns\n",
        "    if tag.startswith('NN'):\n",
        "        cleaned_word = wrd.lower().strip(string.punctuation)  # removing the punctuations & cleaning text\n",
        "        if cleaned_word.isalpha():\n",
        "            topics.append(cleaned_word)\n"
      ],
      "metadata": {
        "id": "pFDJLxNqRxBK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniq_nouns = list(set(topics)) # getting the unique words/topics\n",
        "print(uniq_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7M9secHpB09",
        "outputId": "7c8520f2-9625-445a-e73a-3bbf93337ffa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['task', 't', 'criteria', 'path', 'aviation', 'airplane', 'services', 'newcomer', 'system', 'los', 'quiet', 'altitude', 'procedures', 'hmmh', 'sectors', 'springer', 'resources', 'federal', 'radar', 'increase', 'vector', 'load', 'safety', 'llc', 'april', 'beautiful', 'relief', 'several', 'districts', 'force', 'training', 'waypoints', 'freeway', 'flight', 'public', 'transportation', 'president', 'jet', 'proposal', 'board', 'vice', 'routes', 'bulk', 'consulting', 'northbound', 'proposals', 'tracon', 'wednesday', 'community', 'karpe', 'news', 'terminal', 'weather', 'takeoff', 'hollywood', 'united', 'procedure', 'congress', 'people', 'uproarla', 'destinations', 'quicker', 'march', 'airfield', 'actions', 'attention', 'workload', 'way', 'andres', 'city', 'meetings', 'terms', 'noise', 'village', 'burbank', 'feb', 'senators', 'north', 'conduct', 'studio', 'experts', 'van', 'airlines', 'research', 'request', 'majority', 'ramirez', 'studies', 'east', 'quieter', 'angeles', 'recommendations', 'control', 'airports', 'solutions', 'council', 'gene', 'southern', 'groups', 'pros', 'center', 'explanations', 'region', 'consultant', 'beth', 'things', 'standard', 'association', 'kevin', 'pressure', 'scholten', 'charge', 'approach', 'steeper', 'firm', 'fernando', 'marriott', 'problem', 'lewis', 'navigation', 'n', 'cons', 'mayor', 'encino', 'rate', 'range', 'communities', 'inc', 'diverse', 'reports', 'ffa', 'facilitator', 'air', 'limitations', 'listen', 'sherman', 'neighborhoods', 'airliner', 'congressmen', 'country', 'study', 'nextgen', 'night', 'part', 'action', 'operating', 'others', 'controllers', 'sharon', 'workloads', 'program', 'issue', 'cheers', 'valley', 'pacoima', 'suzanne', 'everybody', 'airport', 'argue', 'representatives', 'held', 'practice', 'climb', 'adam', 's', 'airspace', 'members', 'implementation', 'agreements', 'nuys', 'planes', 'pilots', 'efficient', 'organization', 'message', 'facility', 'issues', 'examples', 'faa', 'audience', 'number', 'plane', 'states', 'feasibility', 'administration', 'neighborhood', 'oaks', 'sixth', 'california', 'diego', 'attendance', 'member', 'san', 'turns', 'past', 'something', 'traffic', 'pieces', 'regulations', 'reindel', 'paths', 'meeting', 'skies', 'group', 'controller', 'wider', 'aircraft', 'conditions', 'month', 'areas', 'director', 'expert', 'generation', 'presentation', 'piece', 'fulton', 'share', 'residents', 'ascent', 'shift', 'nuisance', 'homeowners', 'flights']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tp_freqs = {} # a dictionary to store the topics & their frequencies\n",
        "\n",
        "for noun in uniq_nouns:  # calculating & storing the frequencies using .words.count functionality from textblob\n",
        "    count = blob.words.count(noun, case_sensitive=False)\n",
        "    tp_freqs[noun] = count\n",
        "\n",
        "\n",
        "sorted_tps = sorted(tp_freqs.items(), key=lambda x: x[1], reverse=True)[:15] # sorting the dictionary by frequency to get the top 10 frequent topics\n",
        "\n",
        "print(sorted_tps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbdMamqho-7o",
        "outputId": "9196327e-b689-4e22-cf51-6404a5bc1299"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('faa', 13), ('burbank', 12), ('valley', 11), ('task', 10), ('force', 10), ('noise', 10), ('southern', 10), ('hmmh', 9), ('hollywood', 9), ('recommendations', 9), ('planes', 8), ('san', 8), ('flights', 8), ('flight', 7), ('groups', 7)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"These are the top 15 key topics\")\n",
        "for tp, freq in sorted_tps:\n",
        "    print(f\"Topic: {tp} and it's Frequency is: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFE23NbRpGV8",
        "outputId": "ac1be43d-a9fb-4ce2-82ab-5fd8a8d61601"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "These are the top 15 key topics\n",
            "Topic: faa and it's Frequency is: 13\n",
            "Topic: burbank and it's Frequency is: 12\n",
            "Topic: valley and it's Frequency is: 11\n",
            "Topic: task and it's Frequency is: 10\n",
            "Topic: force and it's Frequency is: 10\n",
            "Topic: noise and it's Frequency is: 10\n",
            "Topic: southern and it's Frequency is: 10\n",
            "Topic: hmmh and it's Frequency is: 9\n",
            "Topic: hollywood and it's Frequency is: 9\n",
            "Topic: recommendations and it's Frequency is: 9\n",
            "Topic: planes and it's Frequency is: 8\n",
            "Topic: san and it's Frequency is: 8\n",
            "Topic: flights and it's Frequency is: 8\n",
            "Topic: flight and it's Frequency is: 7\n",
            "Topic: groups and it's Frequency is: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oNtQEWzpVkQM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Exercise 4"
      ],
      "metadata": {
        "id": "fKoTo0NDqLBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "rgk1mHes-OiS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_amazon = pd.read_csv('amazon_cells_labelled.txt', delimiter='\\t') # loading the datasets\n",
        "data_imdb = pd.read_csv('imdb_labelled.txt', delimiter='\\t')\n",
        "data_yelp = pd.read_csv('yelp_labelled.txt', delimiter='\\t')\n",
        "\n",
        "\n",
        "print(data_amazon.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BYdKPW1-Sr0",
        "outputId": "9fa83b56-0c9b-4236-9d14-06c95159b96e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  So there is no way for me to plug it in here in the US unless I go by a converter.  \\\n",
            "0                        Good case, Excellent value.                                   \n",
            "1                             Great for the jawbone.                                   \n",
            "2  Tied to charger for conversations lasting more...                                   \n",
            "3                                  The mic is great.                                   \n",
            "4  I have to jiggle the plug to get it to line up...                                   \n",
            "\n",
            "   0  \n",
            "0  1  \n",
            "1  1  \n",
            "2  0  \n",
            "3  1  \n",
            "4  0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_amazon.columns = ['Review', 'Label'] # adding headers in each dataset\n",
        "data_imdb.columns = ['Review', 'Label']\n",
        "data_yelp.columns = ['Review', 'Label']"
      ],
      "metadata": {
        "id": "ekZ619SB_S62"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_amazon['Company'] = 'Amazon' # adding a company column\n",
        "data_imdb['Company'] = 'imdb'\n",
        "data_yelp['Company'] = 'yelp'"
      ],
      "metadata": {
        "id": "PR1_aptY_dNi"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_col = pd.concat([data_amazon, data_imdb, data_yelp], ignore_index=True) # combining the datasets\n",
        "\n",
        "comb_data = combined_col # the new dataset\n",
        "\n",
        "print(comb_data.head()) # checking the newdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fty0b--1_xjp",
        "outputId": "dc659d54-ba47-433e-d57e-9abef7cb634a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              Review  Label Company\n",
            "0                        Good case, Excellent value.      1  Amazon\n",
            "1                             Great for the jawbone.      1  Amazon\n",
            "2  Tied to charger for conversations lasting more...      0  Amazon\n",
            "3                                  The mic is great.      1  Amazon\n",
            "4  I have to jiggle the plug to get it to line up...      0  Amazon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "comb_data.to_csv('Sentiment_Analysis_Dataset.csv', index=False) # creating the Sentiment_Analysis_Dataset csv file\n"
      ],
      "metadata": {
        "id": "BOh8m7B7AMKV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(comb_data.columns) # cloumn names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SNyDAUSwAXl4",
        "outputId": "bb217f86-194a-4f5b-da16-d0adf8d5698d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Review', 'Label', 'Company'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(comb_data.isnull().sum()) # checking for nulls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFGnQek5AYvs",
        "outputId": "6205bbe1-3f2b-45d7-bca0-4cb9ea4578aa"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review     0\n",
            "Label      0\n",
            "Company    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm') # importing the STOP_WORDS from spacy and making a list of them\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "stop_words = list(STOP_WORDS)"
      ],
      "metadata": {
        "id": "N_RPoKNyBiuA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "punctuations = string.punctuation # getting the punctuations\n",
        "print(punctuations)\n",
        "\n",
        "def clean_text(txt_dt): # this is the basic function to clean the text\n",
        "\n",
        "    cl_txt = ''.join([char for char in txt_dt if char not in punctuations])\n",
        "    doc = nlp(cl_txt)  # creating a spaCy's parser object for cleaning & tokenizing\n",
        "    tkns = []\n",
        "    for tkn in doc: # filtering the text\n",
        "        if tkn.text.lower() not in stop_words and tkn.text not in punctuations:\n",
        "            tkns.append(tkn.text)\n",
        "    return tkns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV_xJCkIB6b-",
        "outputId": "f5d20dd5-451e-40f8-ab8f-3d1bcea60bee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "class Predictors(TransformerMixin): # the prediction class\n",
        "    def transform(self, X, **transform_params):\n",
        "        return [' '.join(clean_text(text)) for text in X] # joining them back to strings\n",
        "\n",
        "    def fit(self, X, y=None, **fit_params):\n",
        "        return self\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {}"
      ],
      "metadata": {
        "id": "3FZ060mCIR5v"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=stop_words) # using the TfidfVectorizer\n",
        "\n",
        "\n",
        "X = comb_data['Review'] # defining the feature & target\n",
        "y = comb_data['Label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # splitting the data\n"
      ],
      "metadata": {
        "id": "E4YngClAIU9t"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[:5])  # checking training data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1SUSkzOIhjI",
        "outputId": "dfc15113-ff4f-4639-8bc8-89821c7debed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2226    This place is a jewel in Las Vegas, and exactl...\n",
            "1736    The opening sequence of this gem is a classic,...\n",
            "1018    This if the first movie I've given a 10 to in ...\n",
            "891                      Excellent product for the price.\n",
            "2206    The vanilla ice cream was creamy and smooth wh...\n",
            "Name: Review, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pipe_countvect = Pipeline([ ('cleaner', Predictors()),('vectorizer', vectorizer),('classifier', LinearSVC())]) # creating the pipeline\n",
        "\n",
        "\n",
        "pipe_countvect.fit(X_train, y_train) # training the model\n",
        "\n",
        "preds = pipe_countvect.predict(X_test) # predicting with testing data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jlScubdIY1m",
        "outputId": "c8d4b8e2-5738-468c-e3c8-b3e380cfe866"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "    print(f\"{X_test.iloc[i]}, Predictionâ {preds[i]}\")\n",
        "\n",
        "\n",
        "test_acc = accuracy_score(y_test, preds)\n",
        "print(\"Accuracy with test data:\", test_acc)\n",
        "\n",
        "train_pred = pipe_countvect.predict(X_train) # predicting with training data\n",
        "train_acc = accuracy_score(y_train, train_pred)\n",
        "print(\"Accuracy with training data:\", train_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUmNawW4qMFn",
        "outputId": "8708744a-81fb-49d2-ac28-e2fe2d16dd4d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disappointment.. I hate anything that goes in my ear., Predictionâ 0\n",
            "It is a true classic.  , Predictionâ 1\n",
            "Great product., Predictionâ 1\n",
            "This is a great restaurant at the Mandalay Bay., Predictionâ 1\n",
            "It finds my cell phone right away when I enter the car., Predictionâ 1\n",
            "It is simple to use and I like it., Predictionâ 1\n",
            "Of all the dishes, the salmon was the best, but all were great., Predictionâ 1\n",
            "I love the Pho and the spring rolls oh so yummy you have to try., Predictionâ 1\n",
            "Their Research and Development division obviously knows what they're doing., Predictionâ 0\n",
            "Still it's quite interesting and entertaining to follow.  , Predictionâ 1\n",
            "Very poor service., Predictionâ 0\n",
            "Oh yeah, and the storyline was pathetic too.  , Predictionâ 0\n",
            "Strike 2, who wants to be rushed., Predictionâ 0\n",
            "Every element of this story was so over the top, excessively phony and contrived that it was painful to sit through.  , Predictionâ 0\n",
            "The battery works great!, Predictionâ 1\n",
            "I am so tired of clichÃ©s that is just lazy writing, and here they come in thick and fast.  , Predictionâ 1\n",
            "I highly recommend this modest priced cellular phone., Predictionâ 1\n",
            "Waitress was good though!, Predictionâ 1\n",
            "I'm still trying to get over how bad it was.  , Predictionâ 0\n",
            "Don't be afraid of subtitles........ its worth a little aversion therapy 10/10  , Predictionâ 0\n",
            "Accuracy with test data: 0.7959927140255009\n",
            "Accuracy with training data: 0.9867941712204007\n"
          ]
        }
      ]
    }
  ]
}