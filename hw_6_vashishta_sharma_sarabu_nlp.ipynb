{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1029lJbxxnq",
        "outputId": "4c9bbd8e-c5ef-4a93-ac23-f7dcd83ab60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: markovify in /usr/local/lib/python3.10/dist-packages (0.9.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from markovify) (1.3.8)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install markovify pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import markovify, pandas as pd\n",
        "#"
      ],
      "metadata": {
        "id": "lxYYNI01x675"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = pd.read_csv('abcnews-date-text.csv', sep=',') # reading the file (if you get error whlilr running this line of code, just freshly download the file, upload then run it, that worked for me)"
      ],
      "metadata": {
        "id": "GqVtarmSyEpL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inp.head(3))"
      ],
      "metadata": {
        "id": "22U3M4I10dcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67ed789-4fe3-4dd1-f589-3c9cfadb713c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   publish_date                                      headline_text\n",
            "0      20030219  aba decides against community broadcasting lic...\n",
            "1      20030219     act fire witnesses must be aware of defamation\n",
            "2      20030219     a g calls for infrastructure protection summit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = markovify.NewlineText(inp.headline_text, state_size = 2) # initiating the model instance"
      ],
      "metadata": {
        "id": "z9c-czcn0kYk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10): # printing ten randomly-generated sentences using the built model\n",
        "    print(text_model.make_sentence())"
      ],
      "metadata": {
        "id": "hBp7uDg50rjL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcdd065-a262-48a7-8040-29facfced648"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what happened to lars acl the miracle cure that was a child sex accusations now levelled at vic blazes\n",
            "can fish survive lake forbes carp kill in cairns\n",
            "burma arrests two zarqawi aides\n",
            "blues learn from ancient babylon brick by brick wielding teens hold up accused to appear at\n",
            "nt govt accused of drink spiking warning\n",
            "the young but many still on afp to test milk smells\n",
            "abc fact check did the baby faced killer\n",
            "youth concerts to be outsourced\n",
            "police to search for missing plane with 43 passengers crashes in middle east for peacekeeping deloyment in zimbabwe\n",
            "kimberley makes tourism campaign helps boost saleyard cattle numbers have tripled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VKGLz83Q2qNS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gdcpZaJY78hr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Exercise 2"
      ],
      "metadata": {
        "id": "yFZYxsJh3aTF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy\n"
      ],
      "metadata": {
        "id": "OO6okSNp78cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a2415c9-bee8-4e17-8c63-2f62b9f74b2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.66.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2024.8.30)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21691 sha256=3100a6199db7625a265fd3881ca30e4a7c3fbd50310ab33dd5475ef4da7124f6\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=ed865c9832f255ff0f30679ed8d86baad8a551ab91b6e8a2fb3dc4e9746b0c6d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt') # downloading the punkt tokenizer"
      ],
      "metadata": {
        "id": "Gq_YGDRC8Jrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f96cc73-1bc4-4320-feea-edfd472096d1"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "with open('alice.txt', 'r') as f: # getting the text into a variable\n",
        "    text = f.read()\n"
      ],
      "metadata": {
        "id": "HRAkkJEz2qFD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:100])\n"
      ],
      "metadata": {
        "id": "1Zc-4RZj5Dtd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "464ca04e-8813-40cc-9e22-9d556b12dde6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "﻿Alice was beginning to get very tired of sitting by her sister\n",
            "on the bank, and of having nothing t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser = PlaintextParser.from_string(text, Tokenizer('english')) # a parser to prepare the text for summarization\n",
        "\n",
        "summarizer = LsaSummarizer() # using the LSA summarizer, found that it's the best option\n",
        "al_txt_summary = summarizer(parser.document, 5) # getting the summary in 5 sentences\n",
        "\n",
        "for s in al_txt_summary:\n",
        "    print(s)\n"
      ],
      "metadata": {
        "id": "9aiA_x915HX2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026142db-73dc-407d-fc78-ffee99ecd2b5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Mouse did not answer, so Alice went on eagerly:  `There is such a nice little dog near our house I should like to show you!\n",
            "The poor little thing sobbed again (or grunted, it was impossible to say which), and they went on for some while in silence.\n",
            "He sent them word I had not gone (We know it to be true): If she should push the matter on, What would become of you?\n",
            "Don't let him know she liked them best, For this must ever be A secret, kept from all the rest, Between yourself and me.'\n",
            "`If there's no meaning in it,' said the King, `that saves a world of trouble, you know, as we needn't try to find any.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9IDv6x-AGP-6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3"
      ],
      "metadata": {
        "id": "2BVdinbX6J7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "pgod2lbpGxvl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1723064f-016b-46b9-fa41-b6a5bd797172"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text) # getting the sentences from the text\n",
        "print(sentences[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPr9o7Jr3qUF",
        "outputId": "bce3d19d-8a28-438b-a2cf-af27655ef223"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"\\ufeffAlice was beginning to get very tired of sitting by her sister\\non the bank, and of having nothing to do:  once or twice she had\\npeeped into the book her sister was reading, but it had no\\npictures or conversations in it, `and what is the use of a book,'\\nthought Alice `without pictures or conversation?'\", 'So she was considering in her own mind (as well as she could,\\nfor the hot day made her feel very sleepy and stupid), whether\\nthe pleasure of making a daisy-chain would be worth the trouble\\nof getting up and picking the daisies, when suddenly a White\\nRabbit with pink eyes ran close by her.', 'There was nothing so VERY remarkable in that; nor did Alice\\nthink it so VERY much out of the way to hear the Rabbit say to\\nitself, `Oh dear!', 'Oh dear!', \"I shall be late!'\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stop_words = set(stopwords.words('english'))  # getting the stopwords\n",
        "fnl_sents = []\n",
        "for i in sentences:\n",
        "    words = word_tokenize(i) # tokenizing the text\n",
        "    words = [j.lower() for j in words if j.isalpha()] # converting to lowercare& also filtering the unwanted chars\n",
        "    words = [k for k in words if k not in stop_words] # filtering out the stopwords\n",
        "    if words:\n",
        "        fnl_sents.append(' '.join(words)) # takingout the emptystrings\n",
        "\n",
        "\n",
        "print(fnl_sents[:5])"
      ],
      "metadata": {
        "id": "5p7a47OgGP6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b99a493-40b3-46b3-b4c1-e03222a81ad4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['beginning get tired sitting sister bank nothing twice peeped book sister reading pictures conversations use book thought alice without pictures conversation', 'considering mind well could hot day made feel sleepy stupid whether pleasure making would worth trouble getting picking daisies suddenly white rabbit pink eyes ran close', 'nothing remarkable alice think much way hear rabbit say oh dear', 'oh dear', 'shall late']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer() # Vectorizing the words using TfidfVectorizer\n",
        "\n",
        "X = vectorizer.fit_transform(fnl_sents)\n",
        "\n",
        "nmf = NMF(n_components=20, random_state=1).fit(X) # training the NMF\n",
        "\n",
        "idx_to_word = vectorizer.get_feature_names_out() # getting the indices\n",
        "\n",
        "print(idx_to_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpvetwVP4CSR",
        "outputId": "8827c1f4-44fb-41ee-c0a4-668177c07d33"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['abide' 'able' 'absence' ... 'youth' 'zealand' 'zigzag']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, topic in enumerate(nmf.components_): # printing the topics\n",
        "    print(\"Topic {}: {}\".format(i + 1, \",\".join([str(x) for x in idx_to_word[topic.argsort()[-10:]]])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc3dlg6X5GMx",
        "outputId": "895ae74d-b4f1-4e77-944d-ca28af300f75"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic 1: loudly,tone,think,turning,indeed,much,indignantly,nothing,said,alice\n",
            "Topic 2: made,seen,perhaps,looked,might,use,never,mouse,alice,thought\n",
            "Topic 3: looked,first,rabbit,began,way,time,thing,went,little,one\n",
            "Topic 4: soup,old,explain,went,cried,sigh,course,said,mock,turtle\n",
            "Topic 5: mad,fifteenth,mean,went,interrupted,dormouse,angrily,said,hare,march\n",
            "Topic 6: lobsters,question,perhaps,business,replied,mouse,mean,even,mad,know\n",
            "Topic 7: mind,paws,ca,hush,talking,beg,pardon,dinah,dear,oh\n",
            "Topic 8: great,let,fun,beginning,use,must,finished,help,back,come\n",
            "Topic 9: mouth,hookah,called,sternly,see,contemptuously,right,bit,said,caterpillar\n",
            "Topic 10: consider,read,give,evidence,must,may,cat,jury,said,king\n",
            "Topic 11: top,began,screamed,voice,hear,said,play,croquet,shouted,queen\n",
            "Topic 12: doubt,forgot,telescope,might,something,tell,wonder,cats,look,like\n",
            "Topic 13: things,take,remember,tea,first,ca,dormouse,mine,said,hatter\n",
            "Topic 14: added,voice,screamed,whispered,curious,tell,said,cried,went,gryphon\n",
            "Topic 15: witness,cheshire,next,voice,arm,much,cook,baby,said,duchess\n",
            "Topic 16: live,however,bottom,course,say,ah,go,might,dormouse,well\n",
            "Topic 17: triumphantly,louder,jurymen,chorus,jury,alice,another,rabbit,tone,asked\n",
            "Topic 18: going,mournfully,free,tossing,came,impatiently,idea,bright,shook,head\n",
            "Topic 19: see,dinah,join,dance,think,wo,wish,could,cat,would\n",
            "Topic 20: added,heard,better,rate,go,please,say,never,shall,get\n"
          ]
        }
      ]
    }
  ]
}